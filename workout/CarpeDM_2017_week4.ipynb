{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CarpeDM 2017 week 4\n",
    "\n",
    "모두들 추석 잘 보내고 계신가요?<br/>\n",
    "첫번째 강의인 \"Neural Networks and Deep Learning\"이 이번주 부로 종강이네요.<br/>\n",
    "마지막 코딩 과제가 양이 많고 난이도가 있었던만큼, 그리고 다들 명절(+명절에 나온 과제들)도 여유롭게 즐기셔야하니 이번주 활동은 기한을 넉넉히 두기로 했습니다ㅎㅎ 이번 문제를 해결하면서 함께 천천히 첫번째 강의 내용을 복습해보아요!!\n",
    "\n",
    "**시간 복잡도(Time complexity)와 공간 복잡도(Space complexity)**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Big_O_notation <br/>\n",
    "https://en.wikipedia.org/wiki/Time_complexity <br/>\n",
    "http://www.leda-tutorial.org/en/official/ch02s02s03.html <br/>\n",
    "http://ledgku.tistory.com/33 <br/>\n",
    "\n",
    "딥러닝 알고리즘을 구현하는 데 있어 중요한 것은 손실함수(loss function)의 최소화입니다. 그러나 우리는 항상 일정한 시간적·비용적 제약 하에서 알고리즘을 구현해야 합니다. 항상 목표치에 점근적으로 수렴한다는 사실이 증명된 알고리즘보다, 경우에 따라서는 오차의 하계(lower bound)가 주어진 알고리즘을 이용해 문제를 해결하는 편이 더 실용적일 수도 있습니다. 따라서 알고리즘을 짤 때에는 실제로 잘 동작하는지(functionality)와 더불어 어느정도의 시간과 메모리를 필요로 하는지를 확인해보아야 합니다.\n",
    "\n",
    "그러나 복잡한 딥러닝 네트워크의 코드를 한줄한줄 따라가면서 직접 복잡도를 수식으로 계산하는 일은 무척 고단하므로, 우리는 big-O 표기법 대신 여러번의 모의 수행을 통해 각자의 프로세서가 필요로 하는 평균 시간과 메모리를 구해보도록 합시다!\n",
    "\n",
    "**활동 내용**\n",
    "- 첫번째 블록에 그 형태가 주어진 Dense 클래스를 완성해봅시다. Dense 클래스의 세부사항은 다음과 같습니다.\n",
    "    - Dense 객체는 한 노드계층에서 바로 다음 노드계층을 연결해주는 단층 레이어입니다.\n",
    "        - prev, next는 각각 자신을 기준으로 한 직전, 직후의 레이어를 지칭합니다. prev, next를 정의해주는 경우, 해당 Dense 객체의 입력값, 출력값의 수는 직전, 직후의 레이어의 크기에 맞추어 자동으로 결정됩니다.\n",
    "        - 한편 prev, next가 없는 경우 input_dim, output_dim을 통해 직접 입력값, 출력값의 수를 결정해줄 수 있습니다.\n",
    "    - activation은 활성화함수로서, 다음의 세 가지 경우를 상정해 구현해야 합니다.\n",
    "        - linear\n",
    "        - sigmoid\n",
    "        - tanh\n",
    "    - Dense 객체가 가지고 있는 변수들은 다음과 같습니다. <br/> 변수 이름에 self.(이름)을 붙임으로써 해당 객체에 소속된 변수임을 표기할 수 있습니다.\n",
    "        - W, b\n",
    "        - S, V\n",
    "        - gamma, beta1, beta2\n",
    "    - Dense 객체가 가지고 있는 함수들은 다음과 같습니다.\n",
    "        - init : Dense 객체를 생성할 때 호출하는 함수. 즉 Dense( 변수들 ) 이라고 입력하면 해당 함수가 호출됩니다.\n",
    "        - init_parameters : W, b, S, V 등등의 변수들을 초기화합니다.\n",
    "        - forward : $ \\sigma(WX+b) $를 계산해 리턴합니다. $\\sigma$는 활성화함수입니다.\n",
    "        - backward : W, b에 대한 미분값을 리턴합니다.\n",
    "        - update_parameters : 주어진 경사하강법에 따라 변수값을 업데이트합니다.\n",
    "    - 경사하강법은 다음의 세 가지 경우를 상정합니다.\n",
    "        - Momentum\n",
    "        - RMSprop\n",
    "        - Adam\n",
    "\n",
    "- 두번째 블록에서 첫번째 블록에 정의한 내용을 활용해 시간과 메모리를 측정합니다. 이때 loss를 계산하는 부분은 비워두었기에 따로 구현해야합니다. 매 시행마다 시간은 total_time, 메모리는 total_memory에 저장됩니다.\n",
    "\n",
    "- 세번째 블록에서는 두번째 블록에서 하이퍼파라미터를 바꿔가며 기록한 시간과 메모리를 플롯합니다. 바꿔서 실험해볼만한 하이퍼파라미터로는 경사하강법의 종류, 은닉계층의 길이, 트레이닝 데이터 크기 등등이 있습니다. 자유롭게 시험해봅시당\n",
    "\n",
    "\n",
    "**제출 기한 및 방법**\n",
    "- 제출 기한은 10월 15일 일요일 자정까지입니다.\n",
    "- https://github.com/CarpeDM2017/StudyML 에 자신의 이름 이니셜로 생성한 폴더 안에 week3.ipynb 로 저장합니다.\n",
    "- 저장하는 방법은 깃헙 홈페이지나 GitHub Desktop 프로그램, 또는 커맨드 명령어 등을 통해 자유롭게 해주시면 됩니다.\n",
    "- 커맨드 명령어를 이용하고자 하실 경우, 반드시 git push 전에 git pull -> git add -> git commit -> git push 순서로 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, prev=None, next=None):\n",
    "        self.prev = prev\n",
    "        self.next = next\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, prev=None, next=None, input_dim=None, output_dim=None, activation='linear'):\n",
    "        assert prev is not None or input_dim is not None\n",
    "        assert next is not None or output_dim is not None\n",
    "        super(Dense, self).__init__(prev, next)\n",
    "        \n",
    "        if prev is None: self.input_dim = input_dim\n",
    "        else : self.input_dim = self.prev.output_dim\n",
    "        if next is None: self.output_dim = output_dim\n",
    "        else : self.output_dim = self.next.input_dim\n",
    "            \n",
    "        self.W, self.b = self.init_parameters()\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        ### 여기에 코드를 구현해주세요\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ### 여기에 코드를 구현해주세요\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dx):\n",
    "        ### 여기에 코드를 구현해주세요\n",
    "        pass\n",
    "    \n",
    "    def update_parameters(self, dx, method):\n",
    "        ### 여기에 코드를 구현해주세요\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "### Hyperparameters =======================\n",
    "input_size = 10\n",
    "output_size = 10\n",
    "hidden_size = [10,10,10,10]\n",
    "train_size = 10\n",
    "method = 'Momentum'\n",
    "n_iter = 100\n",
    "### =======================================\n",
    "\n",
    "layers = []\n",
    "layers.append(Dense(input_dim=input_size, output_dim=hidden_size[0], activation='tanh'))\n",
    "for i in range(len(hidden_size)-1):\n",
    "    layers.append(Dense(prev=layers[i], output_dim=hidden_size[i+1], activation='tanh'))\n",
    "layers.append(Dense(prev=layers[-1], output_dim=output_size, activation='sigmoid'))\n",
    "\n",
    "input_data = np.random.randn(input_size,train_size)\n",
    "output_data = np.random.randn(output_size,train_size)\n",
    "\n",
    "py = psutil.Process(os.getpid())\n",
    "rss = 0\n",
    "start = time.time()\n",
    "\n",
    "for i in range(n_iter):\n",
    "    x = input_data\n",
    "    for layer in layers:\n",
    "        x = layer.forward(x)\n",
    "\n",
    "    ### 여기에 코드를 구현해주세요.\n",
    "    loss = \n",
    "\n",
    "    dx = loss\n",
    "    for layer in reversed(layers):\n",
    "        dx = layer.backward(dx)\n",
    "        layer.update_parameters(dx, method)\n",
    "    \n",
    "    if py.memory_info()[0] > rss: rss = py.memory_info()[0]\n",
    "\n",
    "end = time.time()\n",
    "total_time = end - start\n",
    "total_memory = rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "### 여기에 각 모의시행 별 소요된 시간, 메모리를 플롯해봅시다\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
