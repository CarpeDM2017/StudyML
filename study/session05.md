## Session 05
### 비선형회귀


* 가법모형
* Support Vector Machine
* 의사결정나무


1. sklearn Iris dataset 분류 문제
  ```Python
  ```


### 가법모형

ㅎㅇ 지난 시간에 이어 여러 모형들을 이어서 공부 ㄱ  
지난 시간에 배운 선형회귀 모형은 사실 가법모형이라는 더 큰 식구들 중 가장 쉬운 놈에 불과하지    

가법모형 = additive model인데 우리가 관찰한 데이터 간의 관계가 실제로는 복잡하더라도  
Kolmogorov-Arnold representation theorem에 의해 단순한 일변수 함수들의 합으로 나타낼 수 있다는 발상임  
Y = f(X1) + f(X2) + ... + f(Xn) 이렇게 f(.)에 대한 선형회귀 형태로 나타나는 모형을 가법모형이라 하고  
g(Y) = f(X1) + f(X2) + ... + f(Xn) 이렇게 Y에도 변형에 가해진 경우 일반화 가법모형이라고 한대  

이때 f와 g의 형태를 우리가 직접적으로 몇개의 모수로 어떻게 이루어져있다고 가정한 다음 추정할 수도 있고  
수식 자체는 어떻게 생겼는지 모르겠지만 이런 특성들을 만족할 것이다라고 간접적으로 가정한 다음 추정할 수도 있음  

전자의 예시는 다항식 회귀, 로지스틱 회귀  
후자의 예시는 cubic smooth spline  


### Support Vector Machine

서포트벡터머신과 의사결정나무는 최근 머신러닝 붐을 일으킨 주범들임  

서포트벡터머신은 원래 이진 분류 문제에서 비롯된 모형임  
데이터를 A와 B 그룹으로 각각 나눌 때 어떻게 하면 그 경계선이 A와 B의 딱 중간에 있도록 할 수 있을까에서 유래한 거 같아  
이때 경계선을 구하고 나면, 실상 각각 A, B로부터 그 경계선에 가장 가까운 점 2개만 빼고 나머지를 다 지운 다음  
경계선을 다시 구해보면 똑같은 게 나옴  
그래서 이 두 점이 경계선을 support 한다고 해서 support vectors라고 하고 support vectors를 찾은 다음  
그 경계선을 구하는 방법이 SVM임  

이진분류 문제를 확장해서 SVM 여러개로 one vs rest, one vs one 등으로 multiclass classification을 함  
또 한편으로는 데이터가 특정 경계선을 중심으로 epsilon 이상 떨어져있지 않다고 하면  
그 경계선에 최대한 내 함수를 맞추는 방식으로 regression이 가능  

그런데 대부분의 경우 A와 B가 딱 잘라서 나뉘어지는 경우가 없고 뒤섞여있자너  
그래서 epsilon에다가 추가로 zeta만큼의 오차까지 허용해주고, 이 zeta를 정규화식으로 넣어줌으로써 학습시킴  
zeta가 없는 앞선 방법론을 hard margin이라고 하고 이걸 soft margin이라고 함  

SVM이 가법모형과 다른 점은  
가법모형은 오차함수가 데이터의 상관관계로부터 얻는 오차 (MSE, Huber 등)이고 제약식은 모수에 걸리는 반면  
SVM은 오차함수가 모수의 크기 + zeta 정규식이고 제약식이 데이터의 오차야  

경계선이 선형이라고 가정할 때가 linear SVM  
여기에 가법모형과 같이 비선형함수 f(.)를 적용해서 적합하면 polynomial SVM, radial SVM 등등   


### 의사결정나무

의사결정나무는 정말 나무 같이 생겼음  
의사결정나무도 SVM과 마찬가지로 이진분류문제에서 비롯되었나봐  
맨 처음 노드(나무의 꼭대기)는 관찰한 데이터의 분포만으로  
우리가 새로운 관측값을 얻었을 때 이게 A인지 B인지를 찍어야하는 상황이야  
이 상황에서의 최선은 둘 중 더 많이 관측된 값으로 찍는 거겠징  

여기서 예컨대 X1값이 10보다 크면 A 아니면 B다와 같이 조건을 하나씩 추가하면서  
새로운 관측값에 대한 정확도를 조금씩 높여나감  
사전에 정한 조건의 개수, 또는 나무의 높이 등등의 기준을 만족하면 모형의 학습이 끝남  

이때 새로 추가할 조건의 내용과 나무 상에서의 위치를 정하는 기준(오차함수)은 크게 3가지가 있음  

첫째는 지니 불순도인데, 나무를 거쳐서 나온 예측값들이 서로 얼마나 뒤섞여있는지를 판단  
지니 불순도를 최소화하는 방향으로 학습함으로써 예측값의 정확도를 높일 수 있음  

둘째는 조건을 추가하기 전의 엔트로피와 조건을 추가한 이후의 엔트로피의 차이를 계산해서 이를 최대화   
즉 엔트로피를 낮추는 방향으로 학습시켜서 불확실성을 줄임  

셋째는 조건을 추가하기 전의 분산과 이후의 분산의 차이를 최대화  


### 참고자료


